---
title: "boston_housing"
output: html_document
author: Sony Nghiem
---

```{r echo=TRUE}
library(MASS)   # for Boston dataset
library(leaps)  # for subset selection
library(glmnet) # for ridge and lasso fit regression
library(pls)    # for pcr and pls
```

Here we are finding the best model to predict the values of housing in Boston
using these following methods: 
1. best subset selection
2. forward stepwise selection
3. backward stepwise selection
4. ridge regression fit
5. lasso regression fit
6. principle component analysis
7. partial least squares analysis

# best subset selection
```{r}
summary(Boston)
regfit = regsubsets(medv~., data=Boston, nvmax=13)
reg.summary = summary(regfit)
reg.summary

names(reg.summary)

par(mfrow=c(2,2))
plot(reg.summary$rss, xlab="No. of variables", ylab="RSS")

plot(reg.summary$adjr2, xlab="No. of variables", ylab="Adjusted Rsq")
which.max(reg.summary$adjr2)
points(11, reg.summary$adjr2[11], col="red", cex=2, pch=20)

plot(reg.summary$cp, xlab="No. of variables", ylab="Cp")
which.min(reg.summary$cp)
points(11, reg.summary$cp[11], col="red", cex=2, pch=20)

plot(reg.summary$bic, xlab="No. of variables", ylab="BIC")
which.min(reg.summary$bic)
points(11, reg.summary$bic[11], col="red", cex=2, pch=20, type="l")

par(mfrow=c(1,1))
plot(regfit, scale="r2")
plot(regfit, scale="adjr2")
plot(regfit, scale="Cp")
plot(regfit, scale="bic")
coef(regfit,11)

```

# forward and backward stepwise selection
```{r}
fwd= regsubsets(medv~., data=Boston, nvmax=13, method="forward")
summary(fwd)
coef(fwd, 11)
bwd= regsubsets(medv~., data=Boston, nvmax=13, method="backward")
summary(bwd)
coef(bwd, 11)
```

# ridge regression
```{r}
set.seed(1)
train = sample(c(TRUE,FALSE),length(Boston$medv)/2, rep=TRUE)
test = !train

x=model.matrix(medv~., Boston)[,-1]
y=Boston$medv

#here I am doing ten-fold cross-validation by default
cv.outcome = cv.glmnet(x[train,], y[train], alpha=0) #for lasso, alpha=1
plot(cv.outcome)
best.lambda = cv.outcome$lambda.min
best.lambda

ridge = glmnet(x[train,], y[train], alpha=0)
ridge.pred = predict(ridge, s=best.lambda, newx=x[test,])
mean((ridge.pred-y[test])^2)
```

# lasso regression
```{r}
cv.outcome = cv.glmnet(x[train,], y[train], alpha=1) 
plot(cv.outcome)
best.lambda = cv.outcome$lambda.min
best.lambda

lasso = glmnet(x[train,], y[train], alpha=1)
lasso.pred = predict(lasso, s=best.lambda, newx=x[test,])
mean((lasso.pred-y[test])^2)
```
It seems like MSEs of both ridge and lasso fit regresson quite similar, ridge is actually a bit smaller.

# Principle Component Analysis
```{r}
set.seed(101)
#here I standardize each predictor and compute 10-fold cross-validation
pcr.fit = pcr(medv~., data=Boston,subset=train, scale=TRUE, validation="CV")
summary(pcr.fit)
validationplot(pcr.fit,val.type = "MSEP")
# I choose M=11
pcr.pred=predict(pcr.fit, x[test,], ncomp=11)
mean((pcr.pred-y[test])^2)

```

# Partial Least Squares Analysis
```{r}
set.seed(10)
#here I standardize each predictor and compute 10-fold cross-validation
pls.fit = plsr(medv~., data=Boston,subset=train, scale=TRUE, validation="CV")
summary(pls.fit)
validationplot(pls.fit,val.type = "MSEP")
# I choose M=11
pls.pred=predict(pls.fit, x[test,], ncomp=11)
mean((pls.pred-y[test])^2)
```

Compare 
```{r}
avg = mean(y[test])
ridge.r2 = 1 - mean((y[test]-ridge.pred)^2)/mean((y[test]-avg)^2)
lasso.r2 = 1 - mean((y[test]-lasso.pred)^2)/mean((y[test]-avg)^2)
pcr.r2 = 1 - mean((y[test]-pcr.pred)^2)/mean((y[test]-avg)^2)
pls.r2 = 1 - mean((y[test]-pls.pred)^2)/mean((y[test]-avg)^2)
barplot(c(ridge.r2, lasso.r2, pcr.r2, pls.r2), col = "turquoise", names.arg = c("Ridge", "Lasso", "PCR", "PLS"), main = "R-squared (Test data)")

```
